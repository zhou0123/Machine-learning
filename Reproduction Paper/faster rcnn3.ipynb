{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import VOCDetection\n",
    "import torch.optim as optim\n",
    "import PIL.Image as Image\n",
    "import PIL.ImageColor as ImageColor\n",
    "import PIL.ImageDraw as ImageDraw\n",
    "import PIL.ImageFont as ImageFont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FONT = ImageFont.truetype('arial.ttf', 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tramforms=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=VOCDetection('C:/数据/目标检测voc2007/',year='2007',image_set='train',download=False,transform=train_tramforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset VOCDetection\n",
       "    Number of datapoints: 2501\n",
       "    Root location: C:/数据/目标检测voc2007/\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       "           )"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'person',\n",
       "  'pose': 'Left',\n",
       "  'truncated': '0',\n",
       "  'difficult': '0',\n",
       "  'bndbox': {'xmin': '185', 'ymin': '62', 'xmax': '279', 'ymax': '199'}},\n",
       " {'name': 'horse',\n",
       "  'pose': 'Left',\n",
       "  'truncated': '0',\n",
       "  'difficult': '0',\n",
       "  'bndbox': {'xmin': '90', 'ymin': '78', 'xmax': '403', 'ymax': '336'}}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[1][1]['annotation']['object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coo_image(train):\n",
    "    data_set=[]\n",
    "    images=[]\n",
    "    for i in range(len(train)):\n",
    "        i+=1\n",
    "        data=[]\n",
    "        for j in range(len(train[i][1]['annotation']['object'])):\n",
    "        \n",
    "            xmin=int(train[i][1]['annotation']['object'][j]['bndbox']['xmin'])\n",
    "            ymin=int(train[i][1]['annotation']['object'][j]['bndbox']['ymin'])\n",
    "            xmax=int(train[i][1]['annotation']['object'][j]['bndbox']['xmax'])\n",
    "            ymax=int(train[i][1]['annotation']['object'][j]['bndbox']['ymax'])\n",
    "            name=train[i][1]['annotation']['object'][j]['name']\n",
    "            data.append([xmin,ymin,xmax,ymax,name])\n",
    "            \n",
    "        image=train[i][0]\n",
    "        data_set.append(data)\n",
    "        \n",
    "        images.append(image)\n",
    "        if(i==1):\n",
    "            break\n",
    "    return data_set,images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset,images=get_coo_image(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=[]\n",
    "for i in range(len(dataset)):\n",
    "    for j in range(len(dataset[i])):\n",
    "        \n",
    "        name.append(dataset[i][j][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=list(set(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    for j in range(len(dataset[i])):\n",
    "        dataset[i][j][-1]=name.index(dataset[i][j][-1])+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vgg16预训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16=vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU(inplace=True)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (25): ReLU(inplace=True)\n",
       "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (27): ReLU(inplace=True)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16=vgg16.features[0:24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gt_box=[x,y,x,y,class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_base_anchors(nn.Module):\n",
    "    def __init__(self,width,height,change_much,ratio=[0.5,1,2],scale=2**torch.arange(3,6)):\n",
    "        super(get_base_anchors,self).__init__()\n",
    "        \n",
    "        \n",
    "        self.width=width\n",
    "        self.height=height\n",
    "        self.change_much=change_much\n",
    "        \n",
    "        self.ratio=ratio\n",
    "        self.scale=scale\n",
    "        \n",
    "    def get_box(self,ws,hs,center_x,center_y):#返回左上，右下\n",
    "        \n",
    "        ws=ws.unsqueeze(1)\n",
    "        hs=hs.unsqueeze(1)\n",
    "        \n",
    "        left=center_x-(ws-1)/2\n",
    "        left=left.reshape(-1,1)\n",
    "        top=center_y-(hs-1)/2\n",
    "        top=top.T\n",
    "        top=top.reshape(-1,1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        right=center_x+(ws-1)/2\n",
    "        right=right.T\n",
    "        right=right.reshape(-1,1)\n",
    "        bottom=center_y+(hs-1)/2\n",
    "        bottom=bottom.T\n",
    "        bottom=bottom.reshape(-1,1)\n",
    "        \n",
    "        box=torch.hstack((left,top,right,bottom))\n",
    "      \n",
    "        return box\n",
    "        \n",
    "        \n",
    "        \n",
    "    def change_on_ratio(self):#产生3个基于ratio的box\n",
    "        ratio=torch.tensor(self.ratio)\n",
    "        base=torch.tensor([0,0,15,15])\n",
    "        \n",
    "        size=(base[2]-base[0]+1)*(base[3]-base[1]+1)\n",
    "        \n",
    "        \n",
    "        ws=base[2]-base[0]+1\n",
    "        hs=base[3]-base[1]+1\n",
    "        \n",
    "        ws=ws*ratio\n",
    "        hs=size/ws\n",
    "        center_x=(base[0]+base[2])/2\n",
    "        center_y=(base[1]+base[3])/2\n",
    "        \n",
    "        \n",
    "        box=self.get_box(ws,hs,center_x,center_y)\n",
    "        #print(box)\n",
    "        \n",
    "        \n",
    "        return box\n",
    "    \n",
    "    def change_on_scale(self,box):\n",
    "        \n",
    "        scale=torch.tensor(self.scale)\n",
    "        scale=scale.unsqueeze(1)\n",
    "        \n",
    "        center_x=(box[0][0]+box[0][2])/2\n",
    "        \n",
    "        center_y=(box[0][1]+box[0][3])/2\n",
    "\n",
    "        ws=box[:,2]-box[:,0]+1#[3,]\n",
    "        hs=box[:,3]-box[:,1]+1\n",
    "        \n",
    "        ws=scale*ws\n",
    "        ws=ws.reshape(-1,1)\n",
    "        hs=scale*hs\n",
    "        hs=hs.reshape(-1,1)\n",
    "        \n",
    "        box=self.get_box(ws,hs,center_x,center_y)\n",
    "        \n",
    "        return box\n",
    "    \n",
    "    def get_all_anchors(self,box):\n",
    "        \n",
    "        \n",
    "        width=self.width\n",
    "        height=self.height\n",
    "        \n",
    "        \n",
    "        \n",
    "        change_much=self.change_much\n",
    "        ws=box[:,2]-box[:,0]+1\n",
    "        hs=box[:,3]-box[:,1]+1\n",
    "        \n",
    "        xs,ys=torch.meshgrid(torch.arange(width),torch.arange(height))\n",
    "        xs=(xs.reshape(-1,1).squeeze()+1)*16\n",
    "        ys=(ys.reshape(-1,1).squeeze()+1)*16\n",
    "        \n",
    "        all_anchors=self.get_box(ws,hs,xs,ys)#[9*n,4]\n",
    " \n",
    "        return all_anchors\n",
    "   \n",
    "  \n",
    "      \n",
    "    def forward(self):\n",
    "        \n",
    "        box=self.change_on_ratio()\n",
    "        box=self.change_on_scale(box)\n",
    "        \n",
    "        box=self.get_all_anchors(box)\n",
    "        return box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_lots_on_base_anchors(nn.Module):\n",
    "    def __init__(self,rpn_cls_score,gt_box,anchors,change_much):\n",
    "        super(get_lots_on_base_anchors,self).__init__()\n",
    "        \n",
    "        self.rpn_cls_score=rpn_cls_score\n",
    "        self.gt_box=gt_box\n",
    "        self.anchors=anchors\n",
    "        self.change_much=change_much\n",
    "        #self.height,self.width=rpn_cls_score.size()[1:3]\n",
    "\n",
    "        \n",
    "    def crop_all_anchors(self,box):#裁剪出去的anchors\n",
    "        \n",
    "        box=box[\n",
    "            (box[:,0]>=0)&(box[:,1]>=0)&(box[:,2]<=self.width*self.change_much)&(box[:,3]<=self.height.change_much)\n",
    "        ]\n",
    "        \n",
    "        return box\n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_iou(self,boxes,gt_anchors):\n",
    "        \n",
    "        N=boxes.size()[0]\n",
    "        K=gt_anchors.size()[0]\n",
    "        \n",
    "        all_iou=torch.zeros((N,K))\n",
    "        \n",
    "        for n in range(N):\n",
    "            size_n=(boxes[n,2]-boxes[n,0]+1)*(boxes[n,3]-boxes[n,1]+1)\n",
    "            \n",
    "            for k in range(K):\n",
    "                \n",
    "                left=max(boxes[n,0],gt_anchors[k,0])\n",
    "                right=min(boxes[n,2],gt_anchors[k,2])\n",
    "                w=right-left+1\n",
    "                \n",
    "                if(w>=0):\n",
    "                    \n",
    "                    top=max(boxes[n,1],gt_anchors[k,1])\n",
    "                    bottom=min(boxes[n,3],gt_anchors[k,3])\n",
    "                    h=bottom-top+1\n",
    "                    \n",
    "                    if(h>=0):\n",
    "                        \n",
    "                        size=w*h\n",
    "                        iou=size/((gt_anchors[k,2]-gt_anchors[k,0]+1)*(gt_anchors[k,3]-gt_anchors[k,1]+1)+size_n-size)\n",
    "                        \n",
    "                        all_iou[n,k]=iou\n",
    "        \n",
    "        return all_iou\n",
    "    def get_targets(self,anchors,gt_box):\n",
    "        \n",
    "        ws=anchors[:,2]-anchors[:,0]+1\n",
    "        hs=anchors[:,3]-anchors[:,1]+1\n",
    "        \n",
    "        centers_x=(anchors[:,0]+anchors[:,2])/2\n",
    "        centers_y=(anchors[:,3]+anchors[:,1])/2\n",
    "        \n",
    "        gt_ws=(gt_box[:,2]-gt_box[:,0])/2\n",
    "        gt_hs=(gt_box[:,3]-gt_box[:,1])/2\n",
    "        \n",
    "        gt_center_x=(gt_box[:,0]+gt_box[:,2])/2\n",
    "        gt_center_y=(gt_box[:,1]+gt_box[:,3])/2\n",
    "        #targets_dw = np.log(gt_widths / ex_widths)\n",
    "\n",
    "        d_x=(gt_center_x-centers_x)/ws\n",
    "        d_x=d_x.reshape(-1,1)\n",
    "        d_y=(gt_center_y-centers_y)/hs\n",
    "        d_y=d_y.reshape(-1,1)     \n",
    "        d_w=torch.log(gt_ws/ws)\n",
    "        d_w=d_w.reshape(-1,1)\n",
    "        d_h=torch.log(gt_hs/hs)\n",
    "        d_h=d_h.reshape(-1,1)\n",
    "        \n",
    "        \n",
    "        targets=torch.hstack((d_x,d_y,d_w,d_h))\n",
    "        \n",
    "        \n",
    "        return targets\n",
    "    \n",
    "        \n",
    "    # def _unmap(data, count, inds, fill=0):\n",
    "    def unmap(self,data,count,inds,fill=0):\n",
    "        \n",
    "        if len(data.size())==1:\n",
    "            ret=torch.empty((count,),dtype=torch.float32)\n",
    "            ret=ret.fill_(fill)\n",
    "            ret[inds]=data\n",
    "            \n",
    "            return ret\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            ret=torch.empty((count,)+data.size()[1:],dtype=torch.float32)\n",
    "            ret=ret.fill_(fill)\n",
    "            ret[inds]=data\n",
    "            \n",
    "            return ret\n",
    "        \n",
    "    def get_lots_im(self):\n",
    "        gt_box=self.gt_box\n",
    "        anchors=self.anchors\n",
    "        rpn_cls_score=self.rpn_cls_score\n",
    "        height,width=rpn_cls_score.size()[2:4]\n",
    "        \n",
    "        h=height\n",
    "        w=width\n",
    "        change_much=self.change_much\n",
    "        \n",
    "        N=anchors.size()[0]\n",
    "        K=gt_box.size()[0]\n",
    "        \n",
    "        labels=-1*torch.ones(N)\n",
    "        box_ind_weight=torch.zeros(N,4,dtype=torch.float64)\n",
    "        box_out_weight=torch.zeros(N,4,dtype=torch.float64)\n",
    "        #对anchors进行第一次筛选\n",
    "        ind=torch.where((anchors[:,0]>=0)\n",
    "                        &(anchors[:,1]>=0)\n",
    "                        &(anchors[:,2]<=width*change_much)\n",
    "                        &(anchors[:,3]<=height*change_much))\n",
    "        \n",
    "        anchors1=anchors[ind]\n",
    "        labels1=labels[ind]\n",
    "        \n",
    "        #对anchors进行第二次筛选\n",
    "        all_iou=self.get_iou(anchors1,gt_box)\n",
    "        K_anchors_index=torch.argmax(all_iou,axis=0)\n",
    "        K_iou=all_iou[K_anchors_index,torch.arange(K)]\n",
    "        \n",
    "        N_anchors_index=torch.argmax(all_iou,axis=1)\n",
    "        N_iou=all_iou[torch.arange(len(labels1)),N_anchors_index]\n",
    "        \n",
    "        \n",
    "        labels1[torch.where(N_iou>0.5)]=1\n",
    "        labels1[torch.where(N_iou<0.2)]=0\n",
    "        labels1[K_anchors_index]=1\n",
    "        targets=self.get_targets(anchors1,gt_box[N_anchors_index,:])\n",
    "        labels=self.unmap(labels1,N,ind,fill=-1)\n",
    "        targets=self.unmap(targets,N,ind,fill=0)\n",
    "        \n",
    "        box_ind_weight[torch.where(labels==1)[0],:]=torch.tensor([1,1,1,1],dtype=torch.float64)\n",
    "        p_weight=torch.empty((4,),dtype=torch.float64)\n",
    "        n_weight=torch.empty((4,),dtype=torch.float64)\n",
    "        p_weight=p_weight.fill_(-1/torch.sum(labels==1,dtype=torch.float64))\n",
    "        n_weight=n_weight.fill_(2/torch.sum(labels==0,dtype=torch.float64))\n",
    "        \n",
    "        box_out_weight[torch.where(labels==1)[0],:]=p_weight\n",
    "        box_out_weight[torch.where(labels==0)[0],:]=n_weight\n",
    "        #print(len(labels))\n",
    "        labels=labels.reshape((1,h,w,9)).permute(0,3,1,2)\n",
    "        labels=labels.reshape(-1)\n",
    "        \n",
    "        targets=targets.reshape((1,9*4,h,w))\n",
    "        box_ind_weight=box_ind_weight.reshape((1,4*9,h,w))\n",
    "        box_out_weight=box_out_weight.reshape((1,4*9,h,w))\n",
    "        \n",
    "        return labels,targets,box_ind_weight,box_out_weight\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_proposal(nn.Module):\n",
    "    def __init__(self,rpn_cls,rpn_bbox,anchors,meet,pre_num,after_num,imgae):\n",
    "        super(get_proposal,self).__init__()\n",
    "        self.meet=meet\n",
    "        self.rpn_cls=rpn_cls\n",
    "        self.rpn_bbox=rpn_bbox\n",
    "        self.anchors=anchors\n",
    "        self.pre_num=pre_num\n",
    "        self.after_num=after_num\n",
    "        self.imgae=imgae\n",
    "        \n",
    "    def get_targets(self,anchors,gt_box):\n",
    "        #print(gt_box)\n",
    "       \n",
    "      \n",
    "        ws=anchors[:,2]-anchors[:,0]+1\n",
    "        hs=anchors[:,3]-anchors[:,1]+1\n",
    "        \n",
    "        centers_x=(anchors[:,0]+anchors[:,2])/2\n",
    "        centers_y=(anchors[:,3]+anchors[:,1])/2\n",
    "        \n",
    "        gt_ws=(gt_box[:,2]-gt_box[:,0])/2\n",
    "        gt_hs=(gt_box[:,3]-gt_box[:,1])/2\n",
    "       \n",
    "        gt_center_x=(gt_box[:,0]+gt_box[:,2])/2\n",
    "        gt_center_y=(gt_box[:,1]+gt_box[:,3])/2\n",
    "        #targets_dw = np.log(gt_widths / ex_widths)\n",
    "\n",
    "        d_x=(gt_center_x-centers_x)/ws\n",
    "        d_x=d_x.reshape(-1,1)\n",
    "        d_y=(gt_center_y-centers_y)/hs\n",
    "        d_y=d_y.reshape(-1,1)     \n",
    "        d_w=torch.log(gt_ws/ws)\n",
    "        d_w=d_w.reshape(-1,1)\n",
    "        d_h=torch.log(gt_hs/hs)\n",
    "        d_h=d_h.reshape(-1,1)\n",
    "       \n",
    "        targets=torch.hstack((d_x,d_y,d_w,d_h))\n",
    "        \n",
    "        return targets\n",
    "        \n",
    "   \n",
    "        \n",
    "    def bbox_and_targets(self,anchors,rpn_bbox_pred):\n",
    "        \n",
    "        ws=anchors[:,2]-anchors[:,0]+1\n",
    "        hs=anchors[:,3]-anchors[:,1]+1\n",
    "        center_x=(anchors[:,0]+anchors[:,2])/2\n",
    "        center_y=(anchors[:,1]+anchors[:,3])/2\n",
    "        \n",
    "        pred_x=rpn_bbox_pred[:,0]*ws+center_x\n",
    "        pred_y=rpn_bbox_pred[:,1]*hs+center_y\n",
    "        pred_w=torch.exp(rpn_bbox_pred[:,2])*ws\n",
    "        pred_h=torch.exp(rpn_bbox_pred[:,3])*hs\n",
    "        \n",
    "        p_box_0=torch.round(pred_x-(pred_w-1)/2).reshape((-1,1))\n",
    "        p_box_1=torch.round(pred_y-(pred_h-1)/2).reshape((-1,1))\n",
    "        p_box_2=torch.round(pred_x+(pred_w-1)/2).reshape((-1,1))\n",
    "        p_box_3=torch.round(pred_y+(pred_h-1)/2).reshape((-1,1))\n",
    "        \n",
    "        p_bbox=torch.hstack((p_box_0,p_box_1,p_box_2,p_box_3))\n",
    "        return p_bbox\n",
    "        \n",
    "    def crop_box(self,box):\n",
    "        \n",
    "        \n",
    "        height,width=self.imgae.size()[2:4]\n",
    "        box[box[:,0]<0]=0\n",
    "        box[box[:,1]<0]=0\n",
    "        box[box[:,2]>width]=width\n",
    "        box[box[:,3]>height]=height\n",
    "        \n",
    "        return box\n",
    "    def nms(self,proposals,scores):\n",
    "        \n",
    "        meet=self.meet\n",
    "        left=proposals[:,0]\n",
    "        top=proposals[:,1]\n",
    "      \n",
    "        right=proposals[:,2]\n",
    "        bottom=proposals[:,3]\n",
    "        \n",
    "        keep=[]\n",
    "        area=(right-left+1)*(bottom-top+1)\n",
    "        index=torch.arange(scores.size()[0])\n",
    "        \n",
    "        while index.size()[0]>0:\n",
    "            \n",
    "            i=index[0]\n",
    "            keep.append(i)\n",
    "           \n",
    "            yy1=torch.maximum(top[i],top[index[1:]])\n",
    "            xx1=torch.maximum(left[i],left[index[1:]])\n",
    "            \n",
    "            xx2=torch.minimum(right[i],right[index[1:]])\n",
    "            yy2=torch.minimum(bottom[i],bottom[index[1:]])\n",
    "            \n",
    "            ins=(xx2-xx1+1)*(yy2-yy1+1)\n",
    "            area1=(right[index[1:]]-left[index[1:]]+1)*(bottom[index[1:]]-top[index[1:]]+1)\n",
    "            area2=(1+bottom[i]-top[i])*(1+right[i]-left[i])\n",
    "            \n",
    "            iou=ins/(area1+area2-ins)\n",
    "            where=torch.where(iou<meet)[0]\n",
    "           \n",
    "            index=index[where+1]\n",
    "            \n",
    "        keep=torch.tensor(keep)\n",
    "        scores=scores[keep]\n",
    "        proposals=proposals[keep]\n",
    "        return scores,proposals,keep\n",
    "            \n",
    "            \n",
    "        \n",
    "    def get_rois(self):\n",
    "        \n",
    "        \n",
    "        anchors=self.anchors\n",
    "        rpn_cls=self.rpn_cls\n",
    "        rpn_bbox=self.rpn_bbox\n",
    "        \n",
    "        rpn_scores=rpn_cls[:,9:,:,:]\n",
    "        rpn_bbox_pred=rpn_bbox.reshape((-1,4))\n",
    "        rpn_scores=rpn_scores.reshape((-1,))\n",
    "        proposals=self.bbox_and_targets(anchors,rpn_bbox_pred)\n",
    "        proposals=self.crop_box(proposals)\n",
    "        #print(len(proposals))\n",
    "\n",
    "        \n",
    "        rpn_scores,indexs=rpn_scores.sort(descending = True)\n",
    "        indexs=indexs.squeeze()\n",
    "        proposals=proposals[indexs,:]\n",
    "       # print(proposals.size())\n",
    "\n",
    "        #预筛选\n",
    "        indexs=indexs[:self.pre_num]\n",
    "        proposals=proposals[:self.pre_num]\n",
    "        rpn_scores=rpn_scores[:self.pre_num]\n",
    "        scores,proposals,keep=self.nms(proposals,rpn_scores)\n",
    "       # print(proposals)\n",
    "        #后筛选\n",
    "        scores=scores[:self.after_num]\n",
    "        proposals=proposals[:self.after_num]\n",
    "        \n",
    "        return proposals,scores,keep\n",
    "    \n",
    "    def get_iou(self,boxes,gt_anchors):\n",
    "        \n",
    "        N=boxes.size()[0]\n",
    "        K=gt_anchors.size()[0]\n",
    "        \n",
    "        all_iou=torch.zeros((N,K))\n",
    "        \n",
    "        for n in range(N):\n",
    "            size_n=(boxes[n,2]-boxes[n,0]+1)*(boxes[n,3]-boxes[n,1]+1)\n",
    "            \n",
    "            for k in range(K):\n",
    "                \n",
    "                left=max(boxes[n,0],gt_anchors[k,0])\n",
    "                right=min(boxes[n,2],gt_anchors[k,2])\n",
    "                w=right-left+1\n",
    "                \n",
    "                if(w>=0):\n",
    "                    \n",
    "                    top=max(boxes[n,1],gt_anchors[k,1])\n",
    "                    bottom=min(boxes[n,3],gt_anchors[k,3])\n",
    "                    h=bottom-top+1\n",
    "                    \n",
    "                    if(h>=0):\n",
    "                        \n",
    "                        size=w*h\n",
    "                        iou=size/((gt_anchors[k,2]-gt_anchors[k,0]+1)*(gt_anchors[k,3]-gt_anchors[k,1]+1)+size_n-size)\n",
    "                        \n",
    "                        all_iou[n,k]=iou\n",
    "        \n",
    "        return all_iou\n",
    "#bbox_target,ind_weight=self.get_final_targets(labels,bbox_target)\n",
    "    def get_final_targets(self,labels,box_target):\n",
    "        bbox_targets=torch.zeros((labels.size()[0],4*20))\n",
    "        ind_weight=torch.zeros(bbox_targets.size(),dtype=torch.float64)\n",
    "        \n",
    "        indexs=torch.where(labels>0)[0]\n",
    "       # print(indexs)\n",
    "        for index in indexs:\n",
    "            cls=labels[index]\n",
    "        \n",
    "            \n",
    "            strat=int(4*cls)\n",
    "            end=strat+4\n",
    "            #print(ind_weight[index,strat:end])\n",
    "            bbox_targets[index,strat:end]=box_target[index]\n",
    "            ind_weight[index,strat:end]=torch.tensor([1,1,1,1],dtype=torch.float64)\n",
    "        return bbox_targets,ind_weight\n",
    "    def for_tarining(self,gt_box):\n",
    "        proposals,scores,keep=self.get_rois()\n",
    "        \n",
    "        all_ious=self.get_iou(proposals,gt_box)#(N,K)\n",
    "        \n",
    "        N_index=all_ious.argmax(axis=1)\n",
    "        N_iou=all_ious[torch.arange(len(N_index)),N_index].reshape(-1,1).squeeze()\n",
    "        \n",
    "        Index_fg=torch.where(N_iou>0.5)\n",
    "        Index_bg=torch.where(N_iou<0.5)\n",
    "        \n",
    "      \n",
    "        indexs=torch.hstack((Index_fg[0],Index_bg[0]))\n",
    "        labels=gt_box[N_index,-1]\n",
    "        labels[len(Index_fg[0]):]=0\n",
    "   \n",
    "        rois=proposals[indexs,:]\n",
    "       \n",
    "     \n",
    "       \n",
    "        bbox_target=self.get_targets(rois,gt_box[N_index])\n",
    "       \n",
    "        \n",
    "        #出现问题\n",
    "        bbox_target,ind_weight=self.get_final_targets(labels,bbox_target)\n",
    "\n",
    "        out_weight=torch.tensor(ind_weight>0,dtype=torch.float64)\n",
    "       # print(torch.where(rois==torch.tensor([0,0,0,0]),device='cpu'))\n",
    "        \n",
    "        \n",
    "        \n",
    "        return rois,labels,bbox_target,ind_weight,out_weight\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_rcnn(1000,images,dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class faster_rcnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(faster_rcnn,self).__init__()\n",
    "        \n",
    "        self.sub_layer1=nn.Conv2d(512,512,kernel_size=3,padding=1,stride=1)\n",
    "        self.sub_relu=nn.ReLU()\n",
    "        \n",
    "        self.layer1_1=nn.Conv2d(512,18,kernel_size=3,padding=1,stride=1)\n",
    "        self.layer1_2=nn.Conv2d(512,36,kernel_size=3,padding=1,stride=1)\n",
    "         \n",
    "        self.linear1=nn.Linear(8192,1000)\n",
    "        self.relu1=nn.ReLU()\n",
    "    \n",
    "        self.linear2=nn.Linear(1000,100)\n",
    "        self.relu2=nn.ReLU()\n",
    "        \n",
    "        self.linear3=nn.Linear(100,4*20)\n",
    "        \n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        \n",
    "        self.linear4=nn.Linear(100,20)\n",
    "        \n",
    "        \n",
    "    def reshape_layer(self,rpn_cls,dim):\n",
    "        \n",
    "        \n",
    "        reshaped=rpn_cls.reshape((1,dim,-1,rpn_cls.size()[-1]))\n",
    "        return reshaped\n",
    "    \n",
    "    def get_roi_feature_maps(self,feature_maps,rois,scale,width,height):\n",
    "        \n",
    "        rois=torch.round(rois/scale)\n",
    "        n,c,w,h=feature_maps.size()\n",
    "        \n",
    "        length=rois.size()[0]\n",
    "        maps=[]\n",
    "        for i in range(length):\n",
    "          \n",
    "            h=int(rois[i,3].detach())-int(rois[i,1].detach())\n",
    "            w=int(rois[i,2].detach())-int(rois[i,0].detach())\n",
    "            \n",
    "            \n",
    "            if h<=0 :\n",
    "                h=1\n",
    "            if w<=0:\n",
    "                w=1\n",
    "           \n",
    "            if rois[i,1]>=height:\n",
    "                rois[i,1]=height-1\n",
    "                h=1\n",
    "            if rois[i,0]>=width:\n",
    "                rois[i,0]=width-1\n",
    "                w=1\n",
    "           \n",
    "            \n",
    "            feature_map=feature_maps[:,:,int(torch.abs(rois[i,1].detach())):int(torch.abs(rois[i,1].detach())+h),int(torch.abs(rois[i,0].detach())):int(torch.abs(rois[i,0].detach())+w)]\n",
    "           \n",
    "            feature_map=Resize([16,16])(feature_map)\n",
    "            \n",
    "            \n",
    "            feature_map=nn.MaxPool2d(kernel_size=(4,4),stride=4,padding=0,dilation=1,ceil_mode=False)(feature_map)\n",
    "            \n",
    "            feature_map=feature_map.reshape(1,-1)\n",
    "            \n",
    "            maps.append(feature_map)\n",
    "            \n",
    "        final=torch.vstack((maps))\n",
    "        return final\n",
    "        \n",
    "   #命名有点问题！！！！ \n",
    "    def bbox_and_targets(self,anchors,rpn_bbox_pred):\n",
    "        \n",
    "        ws=anchors[:,2]-anchors[:,0]+1\n",
    "        hs=anchors[:,3]-anchors[:,1]+1\n",
    "        center_x=(anchors[:,0]+anchors[:,2])/2\n",
    "        center_y=(anchors[:,1]+anchors[:,3])/2\n",
    "        \n",
    "        pred_x=rpn_bbox_pred[:,0]*ws+center_x\n",
    "        pred_y=rpn_bbox_pred[:,1]*hs+center_y\n",
    "        pred_w=torch.exp(rpn_bbox_pred[:,2])*ws\n",
    "        pred_h=torch.exp(rpn_bbox_pred[:,3])*hs\n",
    "        \n",
    "        p_box_0=torch.round(pred_x-(pred_w-1)/2).reshape((-1,1))\n",
    "        p_box_1=torch.round(pred_y-(pred_h-1)/2).reshape((-1,1))\n",
    "        p_box_2=torch.round(pred_x+(pred_w-1)/2).reshape((-1,1))\n",
    "        p_box_3=torch.round(pred_y+(pred_h-1)/2).reshape((-1,1))\n",
    "        \n",
    "        p_bbox=torch.hstack((p_box_0,p_box_1,p_box_2,p_box_3))\n",
    "        return p_bbox\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x,gt_box):\n",
    "        \n",
    "        image=x\n",
    "        x=vgg16(x)\n",
    "        height,width=x.size()[2:]\n",
    "        \n",
    "        x=self.sub_relu(self.sub_layer1(x))\n",
    "        \n",
    "        rpn_cls=self.layer1_1(x)#(1,18,60,40)\n",
    "        rpn_bbox=self.layer1_2(x)#(1,36,60,40)\n",
    "        \n",
    "        rpn_cls_reshaped=self.reshape_layer(rpn_cls,2)\n",
    "        rpn_cls_pred=nn.Softmax(dim=1)(rpn_cls)\n",
    "        rpn_cls=self.reshape_layer(rpn_cls_pred,18)##(1,18,60,40)\n",
    "    \n",
    "    \n",
    "        anchors=get_base_anchors(width,height,16)()   \n",
    "        #anchors=anchors.to('cuda:0')\n",
    "        rpn_labels,rpn_box_target,rpn_ind_weight,rpn_out_weight=get_lots_on_base_anchors(rpn_cls,gt_box,anchors,16).get_lots_im()\n",
    "        rois,labels,bbox_target,ind_weight,out_weight=get_proposal(rpn_cls,rpn_bbox,anchors,0.5,3000,20,image).for_tarining(gt_box)\n",
    "        #print(rois.size())\n",
    "        \n",
    "        final=self.get_roi_feature_maps(x,rois,16,width,height)#(N,X)\n",
    "        N,X=final.size()\n",
    "        final=self.linear1(final)\n",
    "        final=self.relu1(final)\n",
    "        final=self.linear2(final)\n",
    "        final=self.relu2(final)\n",
    "        \n",
    "        target_pred=self.linear3(final)\n",
    "        \n",
    "        cls_prod=self.softmax(self.linear4(final))\n",
    "        \n",
    "        final_labels=cls_prod.argmax(axis=1)\n",
    "       \n",
    "        tar=torch.hstack(((target_pred[torch.arange(len(final_labels)),4*final_labels].reshape((-1,1))),\n",
    "                        (target_pred[torch.arange(len(final_labels)),4*final_labels+1].reshape((-1,1))),\n",
    "                        (target_pred[torch.arange(len(final_labels)),4*final_labels+2].reshape((-1,1))),\n",
    "                        (target_pred[torch.arange(len(final_labels)),4*final_labels+3].reshape((-1,1)))))\n",
    "        \n",
    "        box=self.bbox_and_targets(rois,tar)\n",
    "        \n",
    "        \n",
    "        return box,final_labels,rpn_cls,rpn_bbox,rpn_labels,rpn_box_target,rpn_ind_weight,rpn_out_weight,cls_prod,target_pred,labels,bbox_target,ind_weight,out_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=faster_rcnn()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.00001) #设置优化器和学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rcnn_loss(nn.Module):\n",
    "    def __init__(self,cls_prod,target_pred,labels,bbox_target,ind_weight,out_weight,\n",
    "                rpn_scores,rpn_box_pred,rpn_labels,rpn_box_target,rpn_ind_weight,rpn_out_weight):\n",
    "        super(rcnn_loss,self).__init__()\n",
    "        #final\n",
    "        self.cls_prod=cls_prod\n",
    "        self.target_pred=target_pred\n",
    "        self.labels=labels\n",
    "        self.bbox_target=bbox_target\n",
    "        self.ind_weight=ind_weight\n",
    "        self.out_weight=out_weight\n",
    "        #rpn\n",
    "        self.rpn_scores=rpn_scores\n",
    "        self.rpn_box_pred=rpn_box_pred\n",
    "        self.rpn_labels=rpn_labels\n",
    "        self.rpn_box_target=rpn_box_target\n",
    "        self.rpn_ind_weight=rpn_ind_weight\n",
    "        self.rpn_out_weight=rpn_out_weight\n",
    "   \n",
    "    def l1_smooth(self,target_pred,bbox_target,ind_weight,out_weight):\n",
    "   \n",
    "        \n",
    "        box_diff=target_pred-bbox_target\n",
    "        \n",
    "        in_box_diff=ind_weight*box_diff\n",
    "        abs_in_box_diff=torch.abs(in_box_diff)\n",
    "        \n",
    "        index=abs_in_box_diff<1\n",
    "        L1_loss=torch.pow(abs_in_box_diff,2)*index+(-1*index+1)*(abs_in_box_diff-0.5)\n",
    "        \n",
    "        L1_loss=L1_loss*out_weight\n",
    "        L1_loss=torch.mean(L1_loss)\n",
    "        \n",
    "        return L1_loss\n",
    "    \n",
    "    def labels_loss_for_rpn(self,rpn_labels,rpn_scores):\n",
    "        rpn_labels=rpn_labels\n",
    "        rpn_scores=rpn_scores\n",
    "        rpn_scores=rpn_scores.reshape(-1,2)\n",
    "        rpn_select=torch.where(torch.not_equal(rpn_labels,-1))[0]\n",
    "        rpn_scores=rpn_scores[rpn_select,:]\n",
    "        rpn_labels=rpn_labels[rpn_select]\n",
    "        rpn_scores=nn.Softmax(dim=1)(rpn_scores)\n",
    "        rpn_scores_index=rpn_scores.argmax(axis=1)\n",
    "        \n",
    "        loss=nn.BCEWithLogitsLoss()(rpn_scores[torch.arange(len(rpn_scores_index)),rpn_scores_index],rpn_labels)\n",
    "        \n",
    "        loss=torch.mean(loss)\n",
    "        return loss\n",
    "    def labels_loss_for_final(self,cls_prod,labels):#(N,20)\n",
    "        \n",
    "        cls_prod=cls_prod\n",
    "        labels=labels\n",
    "        cls_prod_index=cls_prod.argmax(axis=1)\n",
    "        \n",
    "        loss=nn.BCEWithLogitsLoss()(cls_prod[torch.arange(len(cls_prod_index)),cls_prod_index],labels.float())\n",
    "        loss=torch.mean(loss)\n",
    "        \n",
    "        return loss\n",
    "    def forward(self):\n",
    "        \n",
    "       \n",
    "        rpn_loss=self.l1_smooth(self.rpn_box_pred,self.rpn_box_target,self.rpn_ind_weight,\n",
    "                                self.rpn_out_weight)+self.labels_loss_for_rpn(self.rpn_labels,self.rpn_scores)\n",
    "        \n",
    "        final_loss=self.l1_smooth(self.target_pred,self.bbox_target,self.ind_weight,\n",
    "                                  self.out_weight)+self.labels_loss_for_final(self.cls_prod,self.labels)\n",
    "        \n",
    "        all_loss=rpn_loss+final_loss\n",
    "        \n",
    "        \n",
    "        return all_loss\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_one(image,xmin,ymin,xmax,ymax,str,font,color='black',thickness=4):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n",
    "    draw.line([(left, top), (left, bottom), (right, bottom),\n",
    "             (right, top), (left, top)], width=thickness, fill=color)\n",
    "    text_bottom = bottom\n",
    "    text_width, text_height = font.getsize(display_str)\n",
    "    margin = np.ceil(0.05 * text_height)\n",
    "    draw.rectangle(\n",
    "      [(left, text_bottom - text_height - 2 * margin), (left + text_width,\n",
    "                                                        text_bottom)],\n",
    "      fill=color)\n",
    "    draw.text(\n",
    "      (left + margin, text_bottom - text_height - margin),\n",
    "      display_str,\n",
    "      fill='black',\n",
    "      font=font)\n",
    "\n",
    "    return image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_it(image,labels,box):\n",
    "    \n",
    "    num_box=box.size()[0]\n",
    "    \n",
    "    image=image.numpy()\n",
    "    \n",
    "    for i in range(num_box):\n",
    "        the_class=labels[i]\n",
    "        xmin,ymin,xmax,ymax=box[i,0],box[i,1],box[i,2],box[i,3]\n",
    "        image=draw_one(image,xmin,ymin,xmax,ymax,'%s'%(the_class),FONT)\n",
    "    image=torch.tensor(image)  \n",
    "    return image    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rcnn(epoches,images,dataset):\n",
    "    \n",
    "    length=len(dataset)\n",
    "    net.train()\n",
    "    for i in range(epoches):\n",
    "        num=0\n",
    "        print(num)\n",
    "\n",
    "        for j in range(len(images)):\n",
    "            optimizer.zero_grad()\n",
    "            image=images[j]\n",
    "            image=image.unsqueeze(0)\n",
    "            gt_box=torch.tensor(dataset[j])\n",
    "          \n",
    "            \n",
    "       #     image = image.to(device)\n",
    "       #     gt_box = gt_box.to(device)\n",
    "            num+=1\n",
    "            box,final_labels,rpn_cls,rpn_bbox,rpn_labels,rpn_box_target,rpn_ind_weight,rpn_out_weight,cls_prod,target_pred,labels,bbox_target,ind_weight,out_weight=net(\n",
    "            image,gt_box)\n",
    "      \n",
    "            \n",
    "            loss=rcnn_loss(cls_prod,target_pred,labels,bbox_target,ind_weight,out_weight,rpn_cls,rpn_bbox,\n",
    "                           rpn_labels,rpn_box_target,rpn_ind_weight,rpn_out_weight)()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print('a'*100)\n",
    "            \n",
    "            \n",
    "            if num%2==0:\n",
    "                \n",
    "                index1=cls_prod.argmax(axis=1)\n",
    "                box_pred2=torch.zeros_like(box_pred[:,0:4])\n",
    "                for i in range(len(box_pred2)):\n",
    "                    box_pred2[i]=box_pred[i,index1[i]:index1[i]+4]\n",
    "                cls_prod=cls_prod[torch.arange(len(index1)),index1]\n",
    "                index2=torch.where(cls_prod>0.8)[0]\n",
    "                cls_prod=torch.round(cls_prod[index2]).type(torch.int)\n",
    "                if len(index2)==0:\n",
    "                    break\n",
    "                else:\n",
    "                    boxes=tools.change_on_pre(rois,box_pred2)[index2,:]\n",
    "                    boxes=torch.round(boxes).type(torch.int)\n",
    "                    list_name=[]\n",
    "                    for i in range(len(cls_prod)):\n",
    "                        list_name.append(name[cls_prod[i]])\n",
    "                    image=draw_it(image,list_name,boxes)\n",
    "                    image=torch.squeeze(image.to('cpu'))\n",
    "                    plt.imshow(image)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-e6d8cfc7c49d>:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  scale=torch.tensor(self.scale)\n",
      "<ipython-input-24-70da47ee8c2f>:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out_weight=torch.tensor(ind_weight>0,dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-127048df2167>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_rcnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-33-5f5c311e245e>\u001b[0m in \u001b[0;36mtrain_rcnn\u001b[1;34m(epoches, images, dataset)\u001b[0m\n\u001b[0;32m     17\u001b[0m        \u001b[1;31m#     gt_box = gt_box.to(device)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mnum\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             box,final_labels,rpn_cls,rpn_bbox,rpn_labels,rpn_box_target,rpn_ind_weight,rpn_out_weight,cls_prod,target_pred,labels,bbox_target,ind_weight,out_weight=net(\n\u001b[0m\u001b[0;32m     20\u001b[0m             image,gt_box)\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-85c10aaf2450>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, gt_box)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;31m#print(rois.size())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_roi_feature_maps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrois\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#(N,X)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-85c10aaf2450>\u001b[0m in \u001b[0;36mget_roi_feature_maps\u001b[1;34m(self, feature_maps, rois, scale, width, height)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0mh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrois\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrois\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m             \u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrois\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrois\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "train_rcnn(100,images,dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
