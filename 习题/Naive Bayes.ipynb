{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('./相关资料/Reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 选取good bad 评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_good_bad_data(object):\n",
    "    '''\n",
    "    分数为5为good\n",
    "    分数为1为bad\n",
    "    '''\n",
    "    def __init__(self,data):\n",
    "        self.data=data\n",
    "        \n",
    "    def get_two_groups(self):\n",
    "        '''\n",
    "        选取分数最高，而且赞同率100%的样本为正样本\n",
    "        选取分数最低，而且赞同率100%的样本为负样本\n",
    "        '''\n",
    "        opts=np.array(self.data.loc[0:,['HelpfulnessNumerator','HelpfulnessDenominator','Score']])\n",
    "        opts[:,0:-1]+=1\n",
    "        index_good=np.where((opts[:,-1]==5) * (opts[:,0]/opts[:,1]==1)*(opts[:,0]!=1 )* (opts[:,1]!=1))\n",
    "        index_bad=np.where((opts[:,-1]==1) * (opts[:,0]/opts[:,1]==1) * (opts[:,0]!=1 )* (opts[:,1]!=1))\n",
    "\n",
    "        return index_bad,index_good\n",
    "    \n",
    "    def get_two_summary(self):\n",
    "        \n",
    "        index_bad,index_good=self.get_two_groups()\n",
    "        index_bad,index_good=index_bad[0],index_good[0]\n",
    "        num=len(np.array(self.data))\n",
    "        index_other=np.array(range(num))\n",
    "   \n",
    "        inds=np.concatenate((index_good,index_bad))\n",
    "        index_other=np.delete(index_other,inds)\n",
    "        \n",
    "        \n",
    "        \n",
    "        good_summary=np.array(self.data.loc[0:,['Summary']])[index_good]\n",
    "        bad_summary=np.array(self.data.loc[0:,['Summary']])[index_bad]\n",
    "        \n",
    "        test_summary=np.array(self.data.loc[0:,['Summary']])[index_other]\n",
    "        \n",
    "        scores=np.array(self.data.loc[0:,['Score']])[index_other]\n",
    "        \n",
    "        return good_summary,index_good,bad_summary,index_bad,scores,test_summary,index_other\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_summary1,index_good,bad_summary1,index_bad,scores,test_summary,index_other=get_good_bad_data(data).get_two_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成训练集，测试集，和标签"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练集正样本1万，负样本8千，测试集正样本3000，负样本2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_summary=np.concatenate((good_summary1,bad_summary1),axis=0)\n",
    "good_summary=good_summary1[0:10000]\n",
    "bad_summary=bad_summary1[0:8000]\n",
    "test_=np.concatenate((good_summary1[10000:13000],bad_summary1[8000:10000]))\n",
    "labels=np.zeros(len(test_))\n",
    "labels[0:3000]+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 伯努利朴素贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes(object):\n",
    "    \n",
    "    def __init__(self,good_summary,bad_summary):\n",
    "        \n",
    "        self.sen_prod_good=None\n",
    "        self.sen_prod_bad=None\n",
    "        \n",
    "        self.prod_good=None\n",
    "        self.prod_bad=None\n",
    "        \n",
    "        self.good_summary=good_summary\n",
    "        self.bad_summary=bad_summary\n",
    "        \n",
    "    def train(self):\n",
    "        good_num=len(self.good_summary)\n",
    "        all_summary=np.concatenate((self.good_summary,self.bad_summary),axis=0)\n",
    "        labels=np.zeros(len(all_summary))\n",
    "        labels[0:good_num]+=1\n",
    "        \n",
    "        self.prod_good=good_num/len(all_summary)\n",
    "        self.prod_bad=1-self.prod_good\n",
    "        \n",
    "        words=self.get_word_vec()\n",
    "        \n",
    "        self.sen_prod_good=np.zeros(len(words))+0.001\n",
    "        self.sen_prod_bad=np.zeros(len(words))+0.001\n",
    "        sum1=0\n",
    "        sum0=0\n",
    "        \n",
    "        for i in range(len(all_summary)):  \n",
    "            if labels[i]==1:\n",
    "                vec=self.summary_to_vec(nltk.word_tokenize(all_summary[i][0]),words)\n",
    "                self.sen_prod_good+=vec\n",
    "                sum1+=np.sum(vec)\n",
    "            if labels[i]==0:\n",
    "                vec=self.summary_to_vec(nltk.word_tokenize(all_summary[i][0]),words)\n",
    "                self.sen_prod_bad+=vec\n",
    "                sum0+=np.sum(vec)\n",
    "                \n",
    "        \n",
    "        self.sen_prod_bad=np.log(self.sen_prod_bad/sum0)\n",
    "        self.sen_prod_good=np.log(self.sen_prod_good/sum1)\n",
    "        \n",
    "    def predict(self, others):\n",
    "        \n",
    "        self.train()\n",
    "        \n",
    "        words=self.get_word_vec()\n",
    "        \n",
    "        all_num=len(others)\n",
    "        pre_list=[]\n",
    "        for i in range(all_num):\n",
    "            vec=self.summary_to_vec(nltk.word_tokenize(others[i][0]),words)\n",
    "           \n",
    "            p_good=np.sum(self.sen_prod_good*vec)+np.log(self.prod_good)\n",
    "            p_bad=np.sum(self.sen_prod_bad*vec)+np.log(self.prod_bad)\n",
    "            \n",
    "            if p_bad>=p_good:\n",
    "                pre_list.append(0)\n",
    "            if p_bad<p_good:\n",
    "                pre_list.append(1)\n",
    "                \n",
    "        return np.array(pre_list)\n",
    "        \n",
    "                \n",
    "        \n",
    "    def get_word_vec(self):\n",
    "        all_summary=np.concatenate((self.good_summary,self.bad_summary),axis=0)\n",
    "        list_words=[]\n",
    "        for i in range(len(all_summary)):\n",
    "            list_words=list_words+nltk.word_tokenize(all_summary[i][0])\n",
    "        words=list(set(list_words))\n",
    "        return words\n",
    "    \n",
    "    def summary_to_vec(self,summary,words):\n",
    "        \n",
    "        '''\n",
    "        summary:列表\n",
    "        '''\n",
    "        \n",
    "        vec=np.zeros(len(words))\n",
    "        \n",
    "        for word in summary:\n",
    "            \n",
    "            if  word in words:\n",
    "                indexs=words.index(word)\n",
    "                if vec[indexs]==0:\n",
    "                    vec[indexs]+=1\n",
    "        \n",
    "        return vec\n",
    "    \n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正确率预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_=NaiveBayes(good_summary,bad_summary).predict(test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8778\n"
     ]
    }
   ],
   "source": [
    "print(sum(pre_==labels)/len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
